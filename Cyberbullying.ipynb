{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Layer, GRU, Bidirectional, Dense, Input, Reshape, GlobalAveragePooling1D\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./bangla_online_comments_dataset.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "remove_punctuations = [\n",
    "    \"/::\\)\",\"/::\",\"(-_-)\",\"(*_*)\",\"(>_<)\",\":)\",\";)\",\":P\",\"xD\",\"-_-\",\"#\",\"(>_<)\",\"...\",\".\",\",\",\";\",\":\",\"!\",\"?\",\"'\",\"অ�\", \"অাবার\", \"।\",\"?\",\n",
    "    \"\\\"\",\"-\",\"_\",\"/\",\"\\\\\",\"|\",\"{\",\"}\",\"[\",\"]\",\"(\",\")\",\"<\",\">\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"~\",\"`\",\"+\",\"=\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"৳\",\"০\",\n",
    "    \"১\",\"২\",\"৩\",\"৪\",\"৫\",\"৬\",\"৭\",\"৮\",\"৯\",\"\\n\",\"\\t\",\"\\r\",\"\\f\",\"\\v\",\"\\u00C0-\\u017F\",\"\\u2000-\\u206F\",\"\\u25A0-\\u25FF\",\"\\u2600-\\u26FF\",\"\\u2B00-\\u2BFF\",\"\\u3000-\\u303F\",\n",
    "    \"\\uFB00-\\uFB4F\",\"\\uFE00-\\uFE0F\",\"\\uFE30-\\uFE4F\",\"\\u1F600-\\u1F64F\",\"\\u1F300-\\u1F5FF\",\"\\u1F680-\\u1F6FF\",\"\\u1F1E0-\\u1F1FF\",\"\\u2600-\\u26FF\",\"\\u2700-\\u27BF\",\n",
    "    \"\\u1F300-\\u1F5FF\",\"\\u1F900-\\u1F9FF\",\"\\u1F600-\\u1F64F\",\"\\u1F680-\\u1F6FF\",\"\\u1F1E0-\\u1F1FF\",\"\\u1F600-\\u1F64F\",\n",
    "]\n",
    "# reset index of the dataframe\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    text = df.loc[i,'comment']\n",
    "    for punctuation in remove_punctuations:\n",
    "        text = text.replace(punctuation,' ')\n",
    "    df.loc[i,'comment'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji\n",
    "for i in range(len(df)):\n",
    "    text = df.loc[i,'comment']\n",
    "    text = remove_emoji(text)\n",
    "    df.loc[i,'comment'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove english character\n",
    "def remove_english_character(text):\n",
    "    english_character = re.compile(\"[a-zA-Z]+\")\n",
    "    return english_character.sub(r\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove english character\n",
    "for i in range(len(df)):\n",
    "    text = df.loc[i,'comment']\n",
    "    text = remove_english_character(text)\n",
    "    df.loc[i,'comment'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra space\n",
    "def remove_extra_space(text):\n",
    "    extra_space = re.compile(\"\\s+\")\n",
    "    return extra_space.sub(r\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_bengali_character(text):\n",
    "    # Regular expression pattern to match single Bengali characters\n",
    "    single_character = re.compile(r'\\s[ঀ-৿]\\s')\n",
    "    return single_character.sub(\" \", text)\n",
    "\n",
    "# Identify data to check if the remove_single_bengali_character function works\n",
    "for i in range(5):\n",
    "    print(\"Original data:-\\n\", df['comment'][i])\n",
    "    print(\"Processed data:-\\n\", remove_single_bengali_character(df['comment'][i]))\n",
    "    print(\"-----------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(remove_single_bengali_character)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the datasets\n",
    "def explore_data(data):\n",
    "    for i in range(5):\n",
    "        print(\"Sample Comment:-\\n\",data['comment'][i])\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        print(\"Sample Label:-\\n\",data['label'][i])\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "\n",
    "    # analyse the length of text\n",
    "    text_len = [len(text) for text in data['comment']]\n",
    "    print(\"Average length of text:-\",np.mean(text_len))\n",
    "    print(\"Max length of text:-\",np.max(text_len))\n",
    "    print(\"Min length of text:-\",np.min(text_len))\n",
    "    print(\"Standard deviation of length of text:-\",np.std(text_len))\n",
    "    print(\"Median length of text:-\",np.median(text_len))\n",
    "    print(\"25 percentile of length of text:-\",np.percentile(text_len,25))\n",
    "    print(\"50 percentile of length of text:-\",np.percentile(text_len,50))\n",
    "    print(\"75 percentile of length of text:-\",np.percentile(text_len,75))\n",
    "    print(\"100 percentile of length of text:-\",np.percentile(text_len,100))\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "explore_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra space\n",
    "for i in range(len(df)):\n",
    "    text = df.loc[i,'comment']\n",
    "    text = remove_extra_space(text)\n",
    "    df.loc[i,'comment'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number unique words\n",
    "unique_words = set()\n",
    "for comment in df['comment']:\n",
    "    for word in comment.split():\n",
    "        unique_words.add(word)\n",
    "\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of words\n",
    "total_words = [word for comment in df['comment'] for word in comment.split()]\n",
    "print(len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['comment', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler()\n",
    "\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "balanced_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "print(balanced_df['label'].value_counts())\n",
    "\n",
    "balanced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonyms(text):\n",
    "    words = text.split()\n",
    "    augmented_text = []\n",
    "    for word in words:\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if synsets:\n",
    "            synonyms = [synonym for synset in synsets for synonym in synset.lemma_names() if synonym != word]\n",
    "            if synonyms:\n",
    "                synonym = random.choice(synonyms)\n",
    "                augmented_text.append(synonym)\n",
    "            else:\n",
    "                augmented_text.append(word)\n",
    "        else:\n",
    "            augmented_text.append(word)\n",
    "    return ' '.join(augmented_text)\n",
    "\n",
    "def insert_random_word(text):\n",
    "    words = text.split()\n",
    "    num_insertions = min(3, len(words))\n",
    "    for _ in range(num_insertions):\n",
    "        index = random.randint(0, len(words))\n",
    "        words.insert(index, 'random_word')\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_synonyms = balanced_df.copy()\n",
    "augmented_df_synonyms['comment'] = augmented_df_synonyms['comment'].apply(replace_synonyms)\n",
    "augmented_df_synonyms['comment'] = augmented_df_synonyms['comment'].str.replace('random_word', '')\n",
    "\n",
    "augmented_df_random = balanced_df.copy()\n",
    "augmented_df_random['comment'] = augmented_df_random['comment'].apply(insert_random_word)\n",
    "augmented_df_random['comment'] = augmented_df_random['comment'].str.replace('random_word', '')\n",
    "\n",
    "final_df = pd.concat([balanced_df, augmented_df_synonyms, augmented_df_random], ignore_index=True)\n",
    "\n",
    "final_df = final_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure each entry in augmented_comment is a list\n",
    "df['comment'] = df['comment'].apply(lambda x: [str(x)] if not isinstance(x, list) else x)\n",
    "\n",
    "# Join the lists of strings into single strings\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "def bert_tokenizer(text):\n",
    "    tokens = tokenizer.encode_plus(text,\n",
    "                                   add_special_tokens=True,\n",
    "                                   max_length=120,\n",
    "                                   padding='max_length',\n",
    "                                   truncation=True,\n",
    "                                   return_attention_mask=True,\n",
    "                                   return_tensors='np')\n",
    "\n",
    "    input_ids = tokens['input_ids'].tolist()\n",
    "    attention_mask = tokens['attention_mask'].tolist()\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(final_df['comment'], final_df['label'], test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(test_texts, test_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "label_dict = {label: idx for idx, label in enumerate(final_df['label'].unique())}\n",
    "num_classes = len(label_dict)\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels.map(label_dict))\n",
    "val_labels = tf.keras.utils.to_categorical(val_labels.map(label_dict))\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels.map(label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer function to each text entry in the DataFrame\n",
    "train_input_ids, train_attention_mask = zip(*train_texts.map(bert_tokenizer))\n",
    "val_input_ids, val_attention_mask = zip(*val_texts.map(bert_tokenizer))\n",
    "test_input_ids, test_attention_mask = zip(*test_texts.map(bert_tokenizer))\n",
    "\n",
    "# Convert lists to arrays\n",
    "train_input_ids = np.array(train_input_ids)\n",
    "train_attention_mask = np.array(train_attention_mask)\n",
    "val_input_ids = np.array(val_input_ids)\n",
    "val_attention_mask = np.array(val_attention_mask)\n",
    "test_input_ids = np.array(test_input_ids)\n",
    "test_attention_mask = np.array(test_attention_mask)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Train shapes:\", train_input_ids.shape, train_attention_mask.shape, train_labels.shape)\n",
    "print(\"Validation shapes:\", val_input_ids.shape, val_attention_mask.shape, val_labels.shape)\n",
    "print(\"Test shapes:\", test_input_ids.shape, test_attention_mask.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input_ids and attention_mask arrays to remove the extra dimension\n",
    "train_input_ids = np.squeeze(train_input_ids, axis=1)\n",
    "train_attention_mask = np.squeeze(train_attention_mask, axis=1)\n",
    "val_input_ids = np.squeeze(val_input_ids, axis=1)\n",
    "val_attention_mask = np.squeeze(val_attention_mask, axis=1)\n",
    "test_input_ids = np.squeeze(test_input_ids, axis=1)\n",
    "test_attention_mask = np.squeeze(test_attention_mask, axis=1)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Train shapes:\", train_input_ids.shape, train_attention_mask.shape, train_labels.shape)\n",
    "print(\"Validation shapes:\", val_input_ids.shape, val_attention_mask.shape, val_labels.shape)\n",
    "print(\"Test shapes:\", test_input_ids.shape, test_attention_mask.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = Input(shape=(120,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(120,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Define the Transformer block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "def create_transformer_model(input_shape, vocab_size, embed_dim=128, num_heads=2, ff_dim=128, num_transformer_blocks=2, rate=0.1):\n",
    "    inputs = [input_ids, attention_mask] \n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs[0]) \n",
    "    transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_transformer_blocks)]\n",
    "    x = embedding_layer\n",
    "    for transformer_block in transformer_blocks:\n",
    "        x = transformer_block(x, training = True)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (128,)\n",
    "vocab_size = tokenizer.vocab_size + 1\n",
    "\n",
    "transformer_model = create_transformer_model(input_shape, vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with early stopping and learning rate scheduler\n",
    "history_1 = transformer_model.fit(\n",
    "    x=[train_input_ids, train_attention_mask],  \n",
    "    y=train_labels, \n",
    "    epochs = 10,  \n",
    "    batch_size=32,  \n",
    "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss values in a single plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history_1.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history_1.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history_1.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_1.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "test_pred = transformer_model.predict([test_input_ids, test_attention_mask])\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = transformer_model.evaluate([test_input_ids, test_attention_mask], test_labels)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels.argmax(axis=1), test_pred_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "class_names = [\"Not Bully\", \"Troll\", \"Sexual\", \"Religious\", \"Threat\"]\n",
    "\n",
    "# Define true and predicted class names\n",
    "true_class_names = np.array([class_names[label.argmax()] for label in test_labels])\n",
    "predicted_class_names = np.array([class_names[label.argmax()] for label in test_pred])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.argmax(axis=1), test_pred_labels, target_names=class_names))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(true_class_names, predicted_class_names, classes=np.array(class_names),\n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "recall = recall_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "f1 = f1_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(class_names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(class_names)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts):\n",
    "    processed_texts = np.array(list(map(bert_tokenizer, texts)))\n",
    "    input_ids, attention_mask = zip(*processed_texts)\n",
    "    \n",
    "    input_ids = np.squeeze(np.array(input_ids), axis=1)\n",
    "    attention_mask = np.squeeze(np.array(attention_mask), axis=1)\n",
    "    \n",
    "    predictions = transformer_model.predict([input_ids, attention_mask])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=final_df['label'].unique())\n",
    "\n",
    "def explain_instance(text, model_predict):\n",
    "    exp = explainer.explain_instance(text, model_predict, num_features=10)\n",
    "    exp_html = exp.show_in_notebook(text=text)\n",
    "\n",
    "    return exp_html\n",
    "\n",
    "text_to_explain = \"আমি একজন উদার মানুষ হিসেবে পরিচিত হতে চাই। আমি যখন আমার পরিবারের সদস্যদের সাথে সময় কাটাই, তখন আমি সবার কাছে সত্যিকারের সাথে থাকতে চেষ্টা করি। আমি অনেক বিষয়ে আগ্রহী এবং শিখতে চাই। আমি আমার দৈনন্দিন জীবনে নতুন চ্যালেঞ্জ এবং অভিজ্ঞতা অনুভব করতে ভালবাসি। আমার জীবনে সময় কাটানোর প্রধান উপাদান হল পরিবার এবং সামাজিক সম্পর্ক। আমি আমার পরিবারের সদস্যদের সাথে সময় কাটাতে ভালবাসি এবং তাদের সাথে অনেক মজা করি। সাথে থাকা সময়ে আমি সবার কাছে আমার ভাবনা এবং আলোচনা শেয়ার করতে পছন্দ করি।\"\n",
    "explanation = explain_instance(text_to_explain, model_predict)\n",
    "explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_gru_model(input_shape, vocab_size, embed_dim=128, num_heads=4, ff_dim=128, num_transformer_blocks=2, rate=0.1):\n",
    "    inputs = [input_ids, attention_mask]\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs[0])\n",
    "    transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_transformer_blocks)]\n",
    "    x = embedding_layer\n",
    "    for transformer_block in transformer_blocks:\n",
    "        x = transformer_block(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Reshape((-1, 64))(x)\n",
    "    x = GRU(64, return_sequences=True)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (128,)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "transformer_gru_model = create_transformer_gru_model(input_shape, vocab_size)\n",
    "transformer_gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = transformer_gru_model.fit(\n",
    "    x=[train_input_ids, train_attention_mask],\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss values in a single plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history_2.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history_2.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history_2.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_2.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "test_pred = transformer_gru_model.predict([test_input_ids, test_attention_mask])\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = transformer_gru_model.evaluate([test_input_ids, test_attention_mask], test_labels)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels.argmax(axis=1), test_pred_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "class_names = [\"Not Bully\", \"Troll\", \"Sexual\", \"Religious\", \"Threat\"]\n",
    "\n",
    "# Define true and predicted class names\n",
    "true_class_names = np.array([class_names[label.argmax()] for label in test_labels])\n",
    "predicted_class_names = np.array([class_names[label.argmax()] for label in test_pred])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.argmax(axis=1), test_pred_labels, target_names=class_names))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(true_class_names, predicted_class_names, classes=np.array(class_names),\n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "recall = recall_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "f1 = f1_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(class_names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(class_names)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts):\n",
    "    processed_texts = np.array(list(map(bert_tokenizer, texts)))\n",
    "    input_ids, attention_mask = zip(*processed_texts)\n",
    "    \n",
    "    input_ids = np.squeeze(np.array(input_ids), axis=1)\n",
    "    attention_mask = np.squeeze(np.array(attention_mask), axis=1)\n",
    "    \n",
    "    predictions = transformer_gru_model.predict([input_ids, attention_mask])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=final_df['label'].unique())\n",
    "\n",
    "def explain_instance(text, model_predict):\n",
    "    exp = explainer.explain_instance(text, model_predict, num_features=10)\n",
    "    exp_html = exp.show_in_notebook(text=text)\n",
    "\n",
    "    return exp_html\n",
    "\n",
    "text_to_explain = \"তুমি কি সেরা হ্যাকার? কাজের পাঁচ মিনিটে তোর হ্যাক করে দিব। এক বিশ্বাস নিবে?\"\n",
    "explanation = explain_instance(text_to_explain, model_predict)\n",
    "explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, LSTM\n",
    "\n",
    "def create_transformer_gru_model(input_shape, vocab_size, embed_dim=128, num_heads=4, ff_dim=128, num_transformer_blocks=2, rate=0.1):\n",
    "    inputs = [input_ids, attention_mask]\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs[0])\n",
    "    transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_transformer_blocks)]\n",
    "    x = embedding_layer\n",
    "    for transformer_block in transformer_blocks:\n",
    "        x = transformer_block(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Reshape((-1, 64))(x)  # Reshape the output to (batch_size, 1, 64)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (128,)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "transformer_lstm = create_transformer_gru_model(input_shape, vocab_size)\n",
    "transformer_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3 = transformer_lstm.fit(\n",
    "    x=[train_input_ids, train_attention_mask],\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss values in a single plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history_3.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history_3.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history_3.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_3.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "test_pred = transformer_lstm.predict([test_input_ids, test_attention_mask])\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = transformer_lstm.evaluate([test_input_ids, test_attention_mask], test_labels)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels.argmax(axis=1), test_pred_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "class_names = [\"Not Bully\", \"Troll\", \"Sexual\", \"Religious\", \"Threat\"]\n",
    "\n",
    "# Define true and predicted class names\n",
    "true_class_names = np.array([class_names[label.argmax()] for label in test_labels])\n",
    "predicted_class_names = np.array([class_names[label.argmax()] for label in test_pred])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.argmax(axis=1), test_pred_labels, target_names=class_names))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(true_class_names, predicted_class_names, classes=np.array(class_names),\n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "recall = recall_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "f1 = f1_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(class_names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(class_names)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts):\n",
    "    processed_texts = np.array(list(map(bert_tokenizer, texts)))\n",
    "    input_ids, attention_mask = zip(*processed_texts)\n",
    "    \n",
    "    input_ids = np.squeeze(np.array(input_ids), axis=1)\n",
    "    attention_mask = np.squeeze(np.array(attention_mask), axis=1)\n",
    "    \n",
    "    predictions = transformer_lstm.predict([input_ids, attention_mask])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Instantiate a LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=final_df['label'].unique())\n",
    "\n",
    "def explain_instance(text, model_predict):\n",
    "    exp = explainer.explain_instance(text, model_predict, num_features=10)\n",
    "    exp_html = exp.show_in_notebook(text=text)\n",
    "\n",
    "    return exp_html\n",
    "\n",
    "text_to_explain = \"আমি একজন উদার মানুষ হিসেবে পরিচিত হতে চাই। আমি যখন আমার পরিবারের সদস্যদের সাথে সময় কাটাই\"\n",
    "explanation = explain_instance(text_to_explain, model_predict)\n",
    "explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-GRU-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "def create_transformer_gru_model(input_shape, vocab_size, embed_dim=128, num_heads=4, ff_dim=128, num_transformer_blocks=2, rate=0.1):\n",
    "    inputs = [input_ids, attention_mask]\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs[0])\n",
    "    transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_transformer_blocks)]\n",
    "    x = embedding_layer\n",
    "    for transformer_block in transformer_blocks:\n",
    "        x = transformer_block(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Reshape((-1, 64))(x)\n",
    "    x = GRU(64, return_sequences=True)(x)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (128,)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "Hybird_transformer = create_transformer_gru_model(input_shape, vocab_size)\n",
    "Hybird_transformer.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "Hybird_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_4 = Hybird_transformer.fit(\n",
    "    x=[train_input_ids, train_attention_mask],\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss values in a single plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history_4.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history_4.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history_4.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_4.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "test_pred = Hybird_transformer.predict([test_input_ids, test_attention_mask])\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = Hybird_transformer.evaluate([test_input_ids, test_attention_mask], test_labels)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels.argmax(axis=1), test_pred_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "class_names = [\"Not Bully\", \"Troll\", \"Sexual\", \"Religious\", \"Threat\"]\n",
    "\n",
    "# Define true and predicted class names\n",
    "true_class_names = np.array([class_names[label.argmax()] for label in test_labels])\n",
    "predicted_class_names = np.array([class_names[label.argmax()] for label in test_pred])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.argmax(axis=1), test_pred_labels, target_names=class_names))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(true_class_names, predicted_class_names, classes=np.array(class_names),\n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "recall = recall_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "f1 = f1_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(class_names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(class_names)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts):\n",
    "    processed_texts = np.array(list(map(bert_tokenizer, texts)))\n",
    "    input_ids, attention_mask = zip(*processed_texts)\n",
    "    \n",
    "    input_ids = np.squeeze(np.array(input_ids), axis=1)\n",
    "    attention_mask = np.squeeze(np.array(attention_mask), axis=1)\n",
    "    \n",
    "    predictions = Hybird_transformer.predict([input_ids, attention_mask])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Instantiate a LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=final_df['label'].unique())\n",
    "\n",
    "def explain_instance(text, model_predict):\n",
    "    exp = explainer.explain_instance(text, model_predict, num_features=10)\n",
    "    exp_html = exp.show_in_notebook(text=text)\n",
    "\n",
    "    return exp_html\n",
    "\n",
    "text_to_explain = \"তুমি কি সেরা হ্যাকার? কাজের পাঁচ মিনিটে তোর হ্যাক করে দিব। এক বিশ্বাস নিবে?\"\n",
    "explanation = explain_instance(text_to_explain, model_predict)\n",
    "explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-BiGRU-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_transformer_model(input_shape, vocab_size, embed_dim=128, num_heads=4, ff_dim=128, num_transformer_blocks=2, rate=0.1):\n",
    "    input_ids = Input(shape=input_shape, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=input_shape, name=\"attention_mask\")\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(input_ids)\n",
    "    \n",
    "    # Transformer blocks\n",
    "    transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_transformer_blocks)]\n",
    "    x = embedding_layer\n",
    "    for transformer_block in transformer_blocks:\n",
    "        x = transformer_block(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Dense layer\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Reshape((-1, 64))(x)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (120,)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "hybrid_transformer = create_hybrid_transformer_model(input_shape, vocab_size)\n",
    "\n",
    "# Create and compile the model\n",
    "hybrid_transformer = create_hybrid_transformer_model(input_shape, vocab_size)\n",
    "hybrid_transformer.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_5 = hybrid_transformer.fit(\n",
    "    x=[train_input_ids, train_attention_mask],\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss values in a single plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history_5.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history_5.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history_5.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_5.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "test_pred = hybrid_transformer.predict([test_input_ids, test_attention_mask])\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = hybrid_transformer.evaluate([test_input_ids, test_attention_mask], test_labels)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels.argmax(axis=1), test_pred_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "class_names = [\"Not Bully\", \"Troll\", \"Sexual\", \"Religious\", \"Threat\"]\n",
    "\n",
    "# Define true and predicted class names\n",
    "true_class_names = np.array([class_names[label.argmax()] for label in test_labels])\n",
    "predicted_class_names = np.array([class_names[label.argmax()] for label in test_pred])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.argmax(axis=1), test_pred_labels, target_names=class_names))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(true_class_names, predicted_class_names, classes=np.array(class_names),\n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "recall = recall_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "f1 = f1_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(class_names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(class_names)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts):\n",
    "    processed_texts = np.array(list(map(bert_tokenizer, texts)))\n",
    "    input_ids, attention_mask = zip(*processed_texts)\n",
    "    \n",
    "    input_ids = np.squeeze(np.array(input_ids), axis=1)\n",
    "    attention_mask = np.squeeze(np.array(attention_mask), axis=1)\n",
    "    \n",
    "    predictions = hybrid_transformer.predict([input_ids, attention_mask])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Instantiate a LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=final_df['label'].unique())\n",
    "\n",
    "def explain_instance(text, model_predict):\n",
    "    exp = explainer.explain_instance(text, model_predict, num_features=10)\n",
    "    exp_html = exp.show_in_notebook(text=text)\n",
    "\n",
    "    return exp_html\n",
    "\n",
    "text_to_explain = \"তুমি কি সেরা হ্যাকার? কাজের পাঁচ মিনিটে তোর হ্যাক করে দিব। এক বিশ্বাস নিবে?\"\n",
    "explanation = explain_instance(text_to_explain, model_predict)\n",
    "explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionalEncoding(Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        position_ids = tf.range(seq_len, dtype=tf.float32)[tf.newaxis, :]\n",
    "        position_encodings = self._get_position_encodings(position_ids)\n",
    "        return position_encodings\n",
    "\n",
    "    def _get_position_encodings(self, position_ids):\n",
    "        angles = 1 / tf.pow(10000, (2 * (tf.range(self.d_model) // 2)) / tf.cast(self.d_model, tf.float32))\n",
    "        positions = tf.einsum('bi,ij->bij', position_ids, angles)\n",
    "        position_encodings = tf.concat([tf.sin(positions[:, :, 0::2]), tf.cos(positions[:, :, 1::2])], axis=-1)\n",
    "        return position_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXLBlock(Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, mem_len):\n",
    "        super(TransformerXLBlock, self).__init__()\n",
    "        self.mem_len = mem_len\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(d_ff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.pos_encoding = RelativePositionalEncoding(self.d_model)\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, memory=None, training=None):\n",
    "        query = inputs\n",
    "\n",
    "        attn_output = self.self_attention(query, query, training=training)\n",
    "\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        attn_output += query\n",
    "        attn_output = self.ln1(attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(attn_output)\n",
    "\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        ffn_output += attn_output\n",
    "        ffn_output = self.ln2(ffn_output)\n",
    "\n",
    "        return ffn_output\n",
    "\n",
    "    def causal_attention_mask(self, query):\n",
    "        seq_length = tf.shape(query)[1]\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_transformerxl_model(input_shape, vocab_size, d_model=128, num_heads=4, d_ff=128, num_blocks=2, rate=0.1):\n",
    "    input_ids = Input(shape=input_shape, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=input_shape, name=\"attention_mask\")\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=d_model)(input_ids)\n",
    "    \n",
    "    transformer_xl_blocks = [TransformerXLBlock(d_model, num_heads, d_ff, rate, mem_len=128) for _ in range(num_blocks)]\n",
    "    x = embedding_layer\n",
    "    for transformer_xl_block in transformer_xl_blocks:\n",
    "        x = transformer_xl_block(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (120,)\n",
    "vocab_size = tokenizer.vocab_size + 1\n",
    "hybrid_transformerxl_model = create_hybrid_transformerxl_model(input_shape, vocab_size)\n",
    "hybrid_transformerxl_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hybrid_transformerxl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_6 = hybrid_transformerxl_model.fit(\n",
    "    x=[train_input_ids, train_attention_mask],\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss values in a single plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history_6.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history_6.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history_6.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_6.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "test_pred = hybrid_transformerxl_model.predict([test_input_ids, test_attention_mask])\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = hybrid_transformerxl_model.evaluate([test_input_ids, test_attention_mask], test_labels)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels.argmax(axis=1), test_pred_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "class_names = [\"Not Bully\", \"Troll\", \"Sexual\", \"Religious\", \"Threat\"]\n",
    "\n",
    "# Define true and predicted class names\n",
    "true_class_names = np.array([class_names[label.argmax()] for label in test_labels])\n",
    "predicted_class_names = np.array([class_names[label.argmax()] for label in test_pred])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.argmax(axis=1), test_pred_labels, target_names=class_names))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(true_class_names, predicted_class_names, classes=np.array(class_names),\n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "recall = recall_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "f1 = f1_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(class_names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(class_names)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts):\n",
    "    processed_texts = np.array(list(map(bert_tokenizer, texts)))\n",
    "    input_ids, attention_mask = zip(*processed_texts)\n",
    "    \n",
    "    input_ids = np.squeeze(np.array(input_ids), axis=1)\n",
    "    attention_mask = np.squeeze(np.array(attention_mask), axis=1)\n",
    "    \n",
    "    predictions = hybrid_transformerxl_model.predict([input_ids, attention_mask])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Instantiate a LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=final_df['label'].unique())\n",
    "\n",
    "def explain_instance(text, model_predict):\n",
    "    exp = explainer.explain_instance(text, model_predict, num_features=10)\n",
    "    exp_html = exp.show_in_notebook(text=text)\n",
    "\n",
    "    return exp_html\n",
    "\n",
    "text_to_explain = \"তুমি কি সেরা হ্যাকার? কাজের পাঁচ মিনিটে তোর হ্যাক করে দিব। এক বিশ্বাস নিবে?\"\n",
    "explanation = explain_instance(text_to_explain, model_predict)\n",
    "explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-XL-BiGRU-BiLSTM(Fusion Transformer-XL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_transformerxl_model(input_shape, vocab_size, d_model=128, num_heads=6, d_ff=128, num_blocks=4, rate=0.1):\n",
    "    input_ids = Input(shape=input_shape, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=input_shape, name=\"attention_mask\")\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=d_model)(input_ids)\n",
    "    \n",
    "    transformer_xl_blocks = [TransformerXLBlock(d_model, num_heads, d_ff, rate, mem_len=128) for _ in range(num_blocks)]\n",
    "    x = embedding_layer\n",
    "    for transformer_xl_block in transformer_xl_blocks:\n",
    "        x = transformer_xl_block(x)\n",
    "    \n",
    "    # Dense layer\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Reshape((-1, 64))(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (120,)\n",
    "vocab_size = tokenizer.vocab_size + 1\n",
    "hybrid_transformerxl = create_hybrid_transformerxl_model(input_shape, vocab_size)\n",
    "hybrid_transformerxl.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hybrid_transformerxl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_7 = hybrid_transformerxl.fit(\n",
    "    x=[train_input_ids, train_attention_mask],\n",
    "    y=train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(history_7.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history_7.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "plt.plot(history_7.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_7.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = hybrid_transformerxl.predict([test_input_ids, test_attention_mask])\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "loss, accuracy = hybrid_transformerxl.evaluate([test_input_ids, test_attention_mask], test_labels)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels.argmax(axis=1), test_pred_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "class_names = [\"Not Bully\", \"Troll\", \"Sexual\", \"Religious\", \"Threat\"]\n",
    "\n",
    "true_class_names = np.array([class_names[label.argmax()] for label in test_labels])\n",
    "predicted_class_names = np.array([class_names[label.argmax()] for label in test_pred])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.argmax(axis=1), test_pred_labels, target_names=class_names))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(true_class_names, predicted_class_names, classes=np.array(class_names),\n",
    "                      title='Confusion Matrix')\n",
    "\n",
    "precision = precision_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "recall = recall_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "f1 = f1_score(test_labels.argmax(axis=1), test_pred_labels, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(class_names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], test_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(class_names)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts):\n",
    "    processed_texts = np.array(list(map(bert_tokenizer, texts)))\n",
    "    input_ids, attention_mask = zip(*processed_texts)\n",
    "    \n",
    "    input_ids = np.squeeze(np.array(input_ids), axis=1)\n",
    "    attention_mask = np.squeeze(np.array(attention_mask), axis=1)\n",
    "    \n",
    "    predictions = hybrid_transformerxl.predict([input_ids, attention_mask])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=final_df['label'].unique())\n",
    "\n",
    "def explain_instance(text, model_predict):\n",
    "    exp = explainer.explain_instance(text, model_predict, num_features=10)\n",
    "    exp_html = exp.show_in_notebook(text=text)\n",
    "\n",
    "    return exp_html\n",
    "\n",
    "text_to_explain = \"তুমি কি সেরা হ্যাকার? কাজের পাঁচ মিনিটে তোর হ্যাক করে দিব। এক বিশ্বাস নিবে?\"\n",
    "explanation = explain_instance(text_to_explain, model_predict)\n",
    "explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to calculate metrics\n",
    "class MetricsCallback(Callback):\n",
    "    def __init__(self, val_data):\n",
    "        super(MetricsCallback, self).__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.precisions = []\n",
    "        self.recalls = []\n",
    "        self.f1s = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_pred = np.argmax(self.model.predict(self.validation_data[0]), axis=1)\n",
    "        val_true = np.argmax(self.validation_data[1], axis=1)\n",
    "        \n",
    "        precision = precision_score(val_true, val_pred, average='weighted')\n",
    "        recall = recall_score(val_true, val_pred, average='weighted')\n",
    "        f1 = f1_score(val_true, val_pred, average='weighted')\n",
    "        accuracy = accuracy_score(val_true, val_pred)\n",
    "        \n",
    "        self.precisions.append(precision)\n",
    "        self.recalls.append(recall)\n",
    "        self.f1s.append(f1)\n",
    "        self.accuracies.append(accuracy)\n",
    "        \n",
    "        print(f' - val_accuracy: {accuracy:.4f} - val_precision: {precision:.4f} - val_recall: {recall:.4f} - val_f1: {f1:.4f}')\n",
    "\n",
    "# Initialize a list to store the history of each fold\n",
    "fold_histories = []\n",
    "\n",
    "k_folds =5\n",
    "\n",
    "# Define the K-fold cross-validator\n",
    "kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics for all folds\n",
    "all_accuracies = []\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1s = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(train_input_ids, np.argmax(train_labels, axis=1))):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    # Split the data into train and validation sets for this fold\n",
    "    fold_train_input_ids, fold_val_input_ids = train_input_ids[train_index], train_input_ids[val_index]\n",
    "    fold_train_attention_mask, fold_val_attention_mask = train_attention_mask[train_index], train_attention_mask[val_index]\n",
    "    fold_train_labels, fold_val_labels = train_labels[train_index], train_labels[val_index]\n",
    "    \n",
    "    # Create the model\n",
    "    hybrid_transformerxl_model = create_hybrid_transformerxl_model(input_shape, vocab_size)\n",
    "    hybrid_transformerxl_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define the metrics callback\n",
    "    metrics_callback = MetricsCallback(([fold_val_input_ids, fold_val_attention_mask], fold_val_labels))\n",
    "    \n",
    "    # Train the model for this fold\n",
    "    fold_history = hybrid_transformerxl_model.fit(\n",
    "        x=[fold_train_input_ids, fold_train_attention_mask],\n",
    "        y=fold_train_labels,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=([fold_val_input_ids, fold_val_attention_mask], fold_val_labels),\n",
    "        callbacks=[early_stopping, metrics_callback]\n",
    "    )\n",
    "    \n",
    "    # Store the metrics for this fold\n",
    "    all_accuracies.extend(metrics_callback.accuracies)\n",
    "    all_precisions.extend(metrics_callback.precisions)\n",
    "    all_recalls.extend(metrics_callback.recalls)\n",
    "    all_f1s.extend(metrics_callback.f1s)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_accuracy = np.mean(all_accuracies)\n",
    "avg_precision = np.mean(all_precisions)\n",
    "avg_recall = np.mean(all_recalls)\n",
    "avg_f1 = np.mean(all_f1s)\n",
    "\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n",
    "print(f'Average Recall: {avg_recall:.4f}')\n",
    "print(f'Average F1-score: {avg_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
